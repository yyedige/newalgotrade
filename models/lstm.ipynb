{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\") # ============================\n",
    "# Configuration\n",
    "# ============================\n",
    "start_date = \"2020-01-01\"\n",
    "sequence_length = 30      # number of past days to look at\n",
    "train_ratio = 0.7         # 80% train, 20% test\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device) # 1. Try relative path from current working directory\n",
    "db_path = Path(\"data\") / \"processed\" / \"data_processed.sqlite\"\n",
    "\n",
    "# If that does not exist, assume we are inside model/ and go one level up\n",
    "if not db_path.exists():\n",
    "    project_root = Path().resolve().parent      # one level up\n",
    "    db_path = project_root / \"data\" / \"processed\" / \"data_processed.sqlite\"\n",
    "\n",
    "print(\"SQLite path:\", db_path)\n",
    "print(\"Exists:\", db_path.exists())\n",
    "if not db_path.exists():\n",
    "    raise FileNotFoundError(f\"Processed SQLite DB not found at {db_path}\")\n",
    "\n",
    "# 2. Parameters\n",
    "start_date = \"2000-01-01\"\n",
    "ticker = \"NVDA\"\n",
    "\n",
    "# 3. Load from SQLite\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    df = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT Date, Close\n",
    "        FROM data\n",
    "        WHERE Date >= ?\n",
    "        ORDER BY Date;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params=(start_date,),\n",
    "        parse_dates=[\"Date\"],\n",
    "    )\n",
    "\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "\n",
    "# 4. Use Date as index and keep Close as price\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "close_prices = df[[\"Close\"]].dropna()\n",
    "\n",
    "print(close_prices.head())\n",
    "print(\"Total rows:\", len(close_prices)) # ============================\n",
    "# 2. Features & scaling\n",
    "# ============================\n",
    "df = close_prices.copy()  # index = Date, column Close\n",
    "\n",
    "# Make sure Close is numeric\n",
    "df[\"Close\"] = pd.to_numeric(df[\"Close\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"Close\"])\n",
    "\n",
    "# Log-returns and technical features\n",
    "df[\"log_ret\"]  = np.log(df[\"Close\"]).diff()\n",
    "df[\"vol_5\"]    = df[\"log_ret\"].rolling(5).std()\n",
    "df[\"vol_20\"]   = df[\"log_ret\"].rolling(20).std()\n",
    "df[\"ma_10\"]    = df[\"Close\"].rolling(10).mean()\n",
    "df[\"ma_30\"]    = df[\"Close\"].rolling(30).mean()\n",
    "df[\"ma_ratio\"] = df[\"ma_10\"] / df[\"ma_30\"]\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "feature_cols = [\"log_ret\", \"vol_5\", \"vol_20\", \"ma_ratio\"]\n",
    "target_col   = \"log_ret\"  # predict next-day log return\n",
    "\n",
    "X_raw = df[feature_cols].values\n",
    "y_raw = df[target_col].shift(-1).values  # next day target\n",
    "\n",
    "# Drop last row where shift(-1) is NaN\n",
    "df    = df.iloc[:-1]\n",
    "X_raw = X_raw[:-1]\n",
    "y_raw = y_raw[:-1]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "\n",
    "# ============================\n",
    "# 3. Sequence creation\n",
    "# ============================\n",
    "def create_sequences(X: np.ndarray, y: np.ndarray, window: int):\n",
    "    \"\"\"\n",
    "    X: shape (N, n_features)\n",
    "    y: shape (N,)\n",
    "    window: number of time steps in the input sequence\n",
    "\n",
    "    Returns:\n",
    "        X_seq: shape (N - window, window, n_features)\n",
    "        y_seq: shape (N - window,)\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - window):\n",
    "        Xs.append(X[i : i + window])\n",
    "        ys.append(y[i + window])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_all, y_all = create_sequences(X_scaled, y_raw, sequence_length)\n",
    "print(\"X_all shape:\", X_all.shape)  # (num_samples, seq_len, n_features)\n",
    "print(\"y_all shape:\", y_all.shape)\n",
    "\n",
    "n_samples, seq_len, n_features = X_all.shape\n",
    "\n",
    "# ============================\n",
    "# 4. Train / test split\n",
    "# ============================\n",
    "train_ratio = 0.7\n",
    "\n",
    "train_end = int(n_samples * train_ratio)\n",
    "\n",
    "X_train_np, y_train_np = X_all[:train_end],  y_all[:train_end]\n",
    "X_test_np,  y_test_np  = X_all[train_end:],  y_all[train_end:]\n",
    "\n",
    "# ============================\n",
    "# 5. Tensors\n",
    "# ============================\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "X_test  = torch.tensor(X_test_np,  dtype=torch.float32).to(device)\n",
    "y_test  = torch.tensor(y_test_np,  dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "\n",
    "print(\"Train samples:\", X_train.shape[0])\n",
    "print(\"Test samples :\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bef214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f19b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6be1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16688e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 6. LSTM model (returns)\n",
    "# ============================\n",
    "class ReturnLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)          # (batch, seq_len, hidden_size)\n",
    "        last_out = out[:, -1, :]       # (batch, hidden_size)\n",
    "        return self.fc(last_out)       # (batch, 1)\n",
    "\n",
    "\n",
    "# Infer number of features from training data\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "# Ensure y_* tensors are (N, 1)\n",
    "if y_train.ndim == 1:\n",
    "    y_train = y_train.view(-1, 1)\n",
    "if y_test.ndim == 1:\n",
    "    y_test = y_test.view(-1, 1)\n",
    "\n",
    "model = ReturnLSTM(\n",
    "    input_size=n_features,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# ============================\n",
    "# 7. Training loop (train/test only)\n",
    "# ============================\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(X_train)\n",
    "    loss = criterion(preds, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:3d} | train={loss.item():.6f}\")\n",
    "\n",
    "# ============================\n",
    "# 8. Evaluation (returns + direction)\n",
    "# ============================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_pred = model(X_train).cpu().numpy().flatten()\n",
    "    test_pred  = model(X_test).cpu().numpy().flatten()\n",
    "\n",
    "y_train_true = y_train.cpu().numpy().flatten()\n",
    "y_test_true  = y_test.cpu().numpy().flatten()\n",
    "\n",
    "def rmse(a, b):\n",
    "    return np.sqrt(((a - b) ** 2).mean())\n",
    "\n",
    "print(f\"Train RMSE (ret): {rmse(y_train_true, train_pred):.6f}\")\n",
    "print(f\"Test  RMSE (ret): {rmse(y_test_true,  test_pred):.6f}\")\n",
    "\n",
    "# Directional accuracy\n",
    "test_dir_acc = (np.sign(test_pred) == np.sign(y_test_true)).mean() * 100\n",
    "print(f\"Test directional accuracy: {test_dir_acc:.2f}%\") # ============================\n",
    "# 9. Reconstruct and plot prices (test)\n",
    "# ============================\n",
    "\n",
    "# 1) ensure numeric dtypes\n",
    "y_test_true = np.asarray(y_test_true, dtype=float).reshape(-1)\n",
    "test_pred   = np.asarray(test_pred,   dtype=float).reshape(-1)\n",
    "\n",
    "if len(y_test_true) != len(test_pred):\n",
    "    raise ValueError(f\"len(y_test_true)={len(y_test_true)} != len(test_pred)={len(test_pred)}\")\n",
    "\n",
    "N = len(y_test_true)\n",
    "if len(close_prices) < N:\n",
    "    raise ValueError(f\"close_prices has only {len(close_prices)} rows, but test set has {N}\")\n",
    "\n",
    "# 2) align dates and actual prices to test horizon\n",
    "test_dates   = close_prices.index[-N:]\n",
    "actual_price = close_prices[\"Close\"].iloc[-N:].values  # shape (N,)\n",
    "\n",
    "# 3) reconstruct prices from log-returns\n",
    "p0 = float(actual_price[0])\n",
    "\n",
    "cum_true = np.cumsum(y_test_true)\n",
    "cum_pred = np.cumsum(test_pred)\n",
    "\n",
    "y_test_price    = p0 * np.exp(cum_true)\n",
    "test_pred_price = p0 * np.exp(cum_pred)\n",
    "\n",
    "# 4) RMSE in price space\n",
    "def rmse(a, b):\n",
    "    return np.sqrt(((a - b) ** 2).mean())\n",
    "\n",
    "test_rmse = rmse(y_test_price, test_pred_price)\n",
    "print(\"Price test RMSE:\", test_rmse)\n",
    "\n",
    "# 5) plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = fig.add_gridspec(4, 1)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[:3, 0])\n",
    "ax1.plot(test_dates, y_test_price,    label=\"Actual Close (reconstructed)\", color=\"blue\")\n",
    "ax1.plot(test_dates, test_pred_price, label=\"Predicted Close\",             color=\"green\")\n",
    "ax1.set_title(f\"{ticker} Close Price Prediction (test)\")\n",
    "ax1.set_ylabel(\"Price\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(gs[3, 0], sharex=ax1)\n",
    "abs_err = np.abs(y_test_price - test_pred_price)\n",
    "ax2.plot(test_dates, abs_err, color=\"red\", label=\"Absolute Error\")\n",
    "ax2.axhline(test_rmse, color=\"black\", linestyle=\"--\",\n",
    "            label=f\"Test RMSE = {test_rmse:.2f}\")\n",
    "ax2.set_xlabel(\"Date\")\n",
    "ax2.set_ylabel(\"Error\")\n",
    "ax2.set_title(\"Prediction Error (price)\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() # ============================\n",
    "# 10. Future forecasting (from feature-based LSTM)\n",
    "# ============================\n",
    "def forecast_future_returns(model, last_X_window, steps=10, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    model: trained ReturnLSTM\n",
    "    last_X_window: np.ndarray of shape (seq_len, n_features) - last training window from X_scaled\n",
    "    steps: number of future steps to roll out\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    window = torch.tensor(last_X_window, dtype=torch.float32).unsqueeze(0).to(device)  # (1, seq_len, n_features)\n",
    "    future_returns = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # model predicts next log-return (batch, 1)\n",
    "            next_ret = model(window)                      # (1, 1)\n",
    "            future_returns.append(next_ret.cpu().numpy()[0, 0])  # scalar\n",
    "\n",
    "            # build a dummy feature vector for the next step:\n",
    "            # we only know the predicted log_ret; other features set to 0 (or last known)\n",
    "            next_feat = np.zeros((1, X_train_np.shape[2]), dtype=np.float32)\n",
    "            next_feat[0, 0] = future_returns[-1]          # put predicted log_ret in first feature\n",
    "\n",
    "            next_feat_t = torch.tensor(next_feat, dtype=torch.float32).to(device)  # (1, n_features)\n",
    "            next_feat_t = next_feat_t.unsqueeze(1)        # (1, 1, n_features)\n",
    "\n",
    "            # roll window\n",
    "            window = torch.cat([window[:, 1:, :], next_feat_t], dim=1)\n",
    "\n",
    "    return np.array(future_returns)  # shape (steps,)\n",
    "\n",
    "\n",
    "# 1) last window of features from your full scaled feature matrix\n",
    "#    use the same X_scaled used to build X_all\n",
    "last_X_window = X_scaled[-sequence_length:]  # (seq_len, n_features)\n",
    "\n",
    "future_days = 10\n",
    "future_log_rets = forecast_future_returns(\n",
    "    model=model,\n",
    "    last_X_window=last_X_window,\n",
    "    steps=future_days,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# 2) reconstruct future prices from last known close\n",
    "close_prices[\"Close\"] = pd.to_numeric(close_prices[\"Close\"], errors=\"coerce\")\n",
    "close_prices = close_prices.dropna(subset=[\"Close\"])\n",
    "\n",
    "last_close = float(close_prices[\"Close\"].iloc[-1])\n",
    "\n",
    "# cumulative predicted log-returns\n",
    "cum_future = np.cumsum(future_log_rets)\n",
    "future_closes = last_close * np.exp(cum_future)  # shape (future_days,)\n",
    "\n",
    "# 3) build future index (business days after last date)\n",
    "last_date = close_prices.index[-1]\n",
    "future_index = pd.bdate_range(start=last_date + pd.Timedelta(days=1),\n",
    "                              periods=future_days)\n",
    "\n",
    "# 4) Plot recent history + forecast\n",
    "history_window = 60\n",
    "history_tail = close_prices.tail(history_window)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_tail.index, history_tail[\"Close\"], label=\"Historical Close\")\n",
    "plt.plot(future_index, future_closes, \"r--\", label=\"Forecasted Close\")\n",
    "plt.title(f\"{ticker} future forecast ({future_days} days)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44bbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba5cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c54110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LSTM for Stock Close Prices\n",
    "# ============================================\n",
    "\n",
    "# 0. Imports & device\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Get project root (go up from models/ folder)\n",
    "BASE_DIR = Path(os.getcwd()).parent\n",
    "\n",
    "# Load config dynamically\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"config\", BASE_DIR / \"config.py\")\n",
    "config = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(config)\n",
    "\n",
    "TICKER = config.TICKER\n",
    "\n",
    "# 1. Load data from SQLite\n",
    "sqlite_path = BASE_DIR / \"data\" / \"processed\" / \"data_processed.sqlite\"\n",
    "\n",
    "print(\"SQLite path:\", sqlite_path)\n",
    "if not sqlite_path.exists():\n",
    "    raise FileNotFoundError(f\"Processed SQLite DB not found at {sqlite_path}\")\n",
    "\n",
    "with sqlite3.connect(sqlite_path) as conn:\n",
    "    df = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT Date, Close\n",
    "        FROM data\n",
    "        ORDER BY Date;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        parse_dates=[\"Date\"],\n",
    "    )\n",
    "\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "close_prices = df[[\"Close\"]].dropna()\n",
    "\n",
    "print(close_prices.head())\n",
    "print(\"Total rows:\", len(close_prices))\n",
    "\n",
    "# 2. Prepare data (prices vector + dates)\n",
    "close_prices[\"Close\"] = pd.to_numeric(close_prices[\"Close\"], errors=\"coerce\")\n",
    "close_prices = close_prices.dropna(subset=[\"Close\"])\n",
    "\n",
    "data = close_prices[\"Close\"].values.reshape(-1, 1)  # (N, 1)\n",
    "dates = close_prices.index\n",
    "print(\"Total points:\", len(data))\n",
    "\n",
    "# 3. Scale & train/test split\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)  # (N, 1)\n",
    "\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]\n",
    "\n",
    "def create_sequences(arr, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(arr) - seq_length):\n",
    "        x = arr[i : i + seq_length]\n",
    "        y = arr[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 60\n",
    "X_train_np, y_train_np = create_sequences(train_data, seq_length)\n",
    "X_test_np, y_test_np = create_sequences(test_data, seq_length)\n",
    "\n",
    "print(\"X_train:\", X_train_np.shape, \"y_train:\", y_train_np.shape)\n",
    "print(\"X_test:\", X_test_np.shape, \"y_test:\", y_test_np.shape)\n",
    "\n",
    "# 4. Tensors\n",
    "X_train = torch.from_numpy(X_train_np).float().to(device)\n",
    "y_train = torch.from_numpy(y_train_np).float().to(device)\n",
    "X_test = torch.from_numpy(X_test_np).float().to(device)\n",
    "y_test = torch.from_numpy(y_test_np).float().to(device)\n",
    "\n",
    "# 5. LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "print(model)\n",
    "\n",
    "# 6. Training\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] Loss: {loss.item():.6f}\")\n",
    "\n",
    "# 7. Predictions in PRICE space\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test_scaled = model(X_test).cpu().numpy()\n",
    "\n",
    "y_test_scaled = y_test.cpu().numpy()\n",
    "\n",
    "preds_test_price = scaler.inverse_transform(preds_test_scaled)\n",
    "y_test_price = scaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_price, preds_test_price))\n",
    "print(f\"Test RMSE (price): {rmse:.4f}\")\n",
    "\n",
    "# 8. Plot actual vs predicted prices\n",
    "test_len = len(y_test_price)\n",
    "test_dates = dates[-test_len:]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates, y_test_price.flatten(), label=\"Actual Close\", color=\"blue\")\n",
    "plt.plot(test_dates, preds_test_price.flatten(), label=\"Predicted Close\", color=\"green\")\n",
    "plt.title(f\"{TICKER} Close Price Prediction (LSTM)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. Simple trading signals from test predictions\n",
    "signals = pd.DataFrame(index=test_dates)\n",
    "signals[\"Actual_Close\"] = y_test_price.flatten()\n",
    "signals[\"Pred_Close\"] = preds_test_price.flatten()\n",
    "\n",
    "signals[\"pred_ret\"] = (signals[\"Pred_Close\"] - signals[\"Actual_Close\"].shift(1)) / signals[\"Actual_Close\"].shift(1)\n",
    "signals.dropna(inplace=True)\n",
    "\n",
    "signals[\"signal\"] = 0\n",
    "signals.loc[signals[\"pred_ret\"] > 0.015, \"signal\"] = 1\n",
    "signals.loc[signals[\"pred_ret\"] < -0.01, \"signal\"] = -1\n",
    "\n",
    "print(signals.head())\n",
    "\n",
    "out_path = BASE_DIR / \"data\" / \"signals\" / \"lstmsignal.csv\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "signals.to_csv(out_path)\n",
    "print(\"Saved historical signals to:\", out_path)\n",
    "\n",
    "# Direction accuracy\n",
    "actual_diff = np.diff(y_test_price.flatten())\n",
    "pred_diff = np.diff(preds_test_price.flatten())\n",
    "actual_dir = np.sign(actual_diff)\n",
    "pred_dir = np.sign(pred_diff)\n",
    "direction_accuracy = np.mean(actual_dir == pred_dir) * 100\n",
    "print(f\"Direction accuracy (up/down): {direction_accuracy:.2f}%\")\n",
    "\n",
    "# 10. Future forecasts (10 days)\n",
    "model.eval()\n",
    "last_sequence = scaled_data[-seq_length:].reshape(1, seq_length, 1)\n",
    "last_sequence_tensor = torch.from_numpy(last_sequence).float().to(device)\n",
    "\n",
    "future_closes = []\n",
    "future_index = pd.date_range(start=dates[-1] + pd.Timedelta(days=1), periods=10, freq='D')\n",
    "\n",
    "current_seq = last_sequence_tensor\n",
    "for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "        next_pred = model(current_seq)\n",
    "    future_closes.append(next_pred.cpu().numpy())\n",
    "    next_pred_np = next_pred.cpu().numpy().reshape(1, 1, 1)\n",
    "    current_seq = torch.cat([current_seq[:, 1:, :], torch.from_numpy(next_pred_np).float().to(device)], dim=1)\n",
    "\n",
    "df_future = pd.DataFrame(\n",
    "    {\"Pred_Close\": scaler.inverse_transform(np.array(future_closes).reshape(-1, 1)).flatten()},\n",
    "    index=future_index,\n",
    ")\n",
    "df_future.index.name = \"Date\"\n",
    "\n",
    "last_actual_close = float(close_prices[\"Close\"].iloc[-1])\n",
    "pred_prices = df_future[\"Pred_Close\"].values\n",
    "\n",
    "pred_returns = np.empty_like(pred_prices, dtype=float)\n",
    "pred_returns[0] = (pred_prices[0] - last_actual_close) / last_actual_close\n",
    "pred_returns[1:] = (pred_prices[1:] - pred_prices[:-1]) / pred_prices[:-1]\n",
    "\n",
    "df_future[\"pred_ret\"] = pred_returns\n",
    "\n",
    "df_future[\"signal\"] = 0\n",
    "df_future.loc[df_future[\"pred_ret\"] > 0.015, \"signal\"] = 1\n",
    "df_future.loc[df_future[\"pred_ret\"] < -0.01, \"signal\"] = -1\n",
    "\n",
    "print(\"\\nForecasts and signals for next 10 days:\")\n",
    "print(df_future)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
